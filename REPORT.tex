\documentclass{UoYCSproject}
\addbibresource{REFERENCES.bib}

\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}

\usepackage{float}

\tikzset{
  ->, % makes the edges directed
  node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
  every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
  initial text=$ $, % sets the text that appears on the start arrow
}

\newenvironment{monospace}{\ttfamily\small}{\par}

\BEng
\supervisor{Dr. Christopher Crispin-Bailey}

\begin{document}

\title{Generation of Hardware Accelerators for an FPGA System}
\author{Jay Valentine}

\maketitle

\begin{abstract}
\end{abstract}

\chapter{Introduction}

\section{Motivation}

With the breakdown of Dennard scaling and the observation of dark silicon in modern microprocessors,
we are rapidly approaching a utilization wall, meaning that not all transistors on a given chip can be utilized
simultaneously \cite{darksilicon}. This has severe consequences for the future of microprocessor architecture,
especially in domains where energy consumption is of great concern, such as the rapidly growing mobile application domain.

Because of this, research is turning towards novel architectures as a solution to this problem.
One such architecture is the \textit{coprocessor-dominated architecture} (\textit{CoDA}), in which one or more general-purpose
processing cores are coupled with a large number of specialised hardware accelerators, which are able to perform very specific
tasks with greater energy efficiency than a general-purpose core.

\section{Aims}

This work aims to show that such an architecture can be utilized in an embedded FPGA platform.
The use of an FPGA allows the accelerators to be designed specifically with the embedded application in mind.
The aim is to produce a tool that can generate hardware accelerators from an application written for the Xilinx MicroBlaze
soft processor \cite{microblaze}.

Because the accelerators are automatically generated by the tool, rather than designed by hand, the architecture
can be re-generated for each version of the application, or even across different applications, very easily.
The use of an FPGA allows the architecture to be very highly specialized, as the FPGA can be re-programmed for each architecture
version. In this sense, the architecture itself becomes an extension of the application software, rather than a static
platform as has traditionally been the case.

\chapter{Literature Review}

\section{Moore's Law and Dennard Scaling}

In 1965, Gordon Moore predicted that the number of transistors in an integrated circuit will double
approximately every two years  \cite{moore}. Similarly, Dennard scaling describes the way in which transistor power density
remains constant as the transistors themselves shrink in size \cite{dennard}. These phenomena combined allow for exponential
transistor-density increases, and this has been exploited to produce exponentially higher performance in
microprocessors year on year.

\begin{figure}[h]
\center{\includegraphics[width=\textwidth]
{figures/moore.png}}
\caption{Moore's 1965 prediction. \cite{moore}}
\end{figure}

However, in more recent times, a breakdown in this scaling has been observed. In the past, Dennard scaling has allowed
microprocessor manufacturers to offset the increased energy cost of faster transistor switching, resulting in higher
and higher clock speeds. However, more recently, microprocessor clock speeds have remained relatively static. This is
due to a breakdown in Dennard scaling for very small transistors, and has caused microprocessor manufacturers to instead
pursue increased performance by the use of multicore designs.

\begin{figure}[h]
\center{\includegraphics[width=\textwidth]
{figures/cpu-speed.png}}
\caption{Trends in microprocessors since 1970. \cite{karlrupp}}
\end{figure}

\section{Dark Silicon}

Dark silicon is a phenomenon which has been observed as transistor density in microprocessors has increased.
It occurs as transistor under-utilization leads to a gap between the speedup observed and that predicted by
extrapolating from historic performance gains.

In \cite{darksilicon}, it is predicted that with 22nm transistors, 21\% of the chip will be dark silicon,
with this rising to 50\% at 8nm. This prediction shows that dark silicon will become a serious limitation
as transistor density grows, especially in areas where energy efficiency is a primary concern.

Dark silicon also becomes a limiting factor with manycore devices. Even when energy consumption is not a
concern, the limited parallelism of most applications results in a dark silicon gap when running with
manycore devices. Again, \cite{darksilicon} shows that beyond a certain number of cores the speedup achieved
is negligible. This is another kind of dark silicon - the underutilization in this case is not a result of
energy concerns but is caused by the limited parallelism of the application being unable to exploit all of
the cores of a device.

There have been several broad responses to this phenomenon.
In \cite{four-horsemen}, Taylor gives two pessimistic predictions regarding the future of silicon
utilization. The first, the 'shrinking horseman', predicts that chips will begin to shrink as a result of
the utilization wall. This would lead to an increase in cost per mm\textsuperscript{2} of silicon, as
design costs, test costs, marketing costs, etc. Put simply, "exponentially smaller chips are not
exponentially smaller".

The second, perhaps slightly less pessimistic, prediction in Taylor's paper
is referred to as 'the dim horseman', and describes the under-clocking or infrequent use of
general-purpose silicon in order to meet power budgets. While a better alternative than the shrinking
of chips, this still causes issues as the chip is no longer operating at maximum capacity.
Taylor outlines several options for the use of this 'dim silicon'. The first, and perhaps most obvious,
is the use of homogenous architectures, but with some cores operating at a lower clock speed, or
even being turned off intermittently. However, there are other 'dim silicon' approaches that make better
use of the unutilized silicon area. One approach is to increase cache sizes, as cache memory is less
power-dense than a processing core would be. This has a secondary benefit as well, in that a larger
cache will reduce the number of cache misses, thereby decreasing the number of power-hungry off-chip
accesses.

Finally, Taylor also describes 'the specialized horseman'. This approach is to use the dark silicon area
not for general-purpose computing, but for a large number of specialized cores, which would be much
more energy efficient than a general-purpose core, allowing for increased energy efficiency at the
cost of silicon area.

\section{Heterogeneous Architectures}

One such 'specialized horseman' approach is the use of heterogeneous architectures.
These are computer architectures in which one or more general-purpose cores are coupled with special-purpose coprocessors,
also known as hardware accelerators. A hardware accelerator is a specialised hardware circuit intended to perform a
specific task more efficiently than a general-purpose processor. While historically hardware accelerators have been designed by
hand for a specific application (e.g. encryption), this is infeasible when considering architectures
with large numbers of accelerators. Thus an automated approach to generating hardware accelerators is required.

This is the approach taken in \cite{high-performance-microarchitecture}. Here Razdan and Smith propose a simple
hardware accelerator architecture which avoids the need for memory synchronization and reduces the overheads involved in
invoking a hardware accelerator. They describe a toolchain which is able to extract instruction streams to be 'outsourced' to an
accelerator core (called a \textit{programmable functional unit}, or \textit{PFU}) after code generation.

\begin{figure}[hbt]
\center{\includegraphics[width=0.75\textwidth]
{figures/pfu.png}}
\caption{Two PFU optimization examples. Both sequences of operations can be evaluated in a single cycle, while the same sequences in MIPS R2000 instructions would take multiple cycles. \cite{high-performance-microarchitecture}}
\end{figure}

The PFU model is a simplification from the general model of a hardware accelerator, which is as a multi-cycle state machine.
Instead each PFU has at most two input operands and at most one output operand. In addition, the PFU-logic instructions (MIPS
instructions that are candidates for translation into a PFU) cannot be memory-access or flow control instructions. This means
that each PFU has an identical interface, and can be executed in a single cycle. This allows PFUs to be executed with a single
instruction, \textit{expfu}, in a single cycle, maintaining the fixed-format and single-cycle instructions of the MIPS
processor's RISC instruction set.

\section{Conservation Cores}

While traditionally hardware accelerators have been used to speed up certain computations, they can also be used to achieve the
same computational performance as a general-purpose processor at a fraction of the energy cost, thus alleviating the dark silicon
problem.

\cite{c-cores} introduces \textit{conservation cores} (\textit{c-cores}), which are hardware accelerators
designed for this purpose. The paper outlines a method for generating c-cores for a given application.
The first step is to identify 'hot' and 'cold' portions of the application, using some form of profiling.
'Hot' code sections are those which are run frequently, and so are ideal candidates for c-cores. 'Cold' sections
are run infrequently, and so are not ideal candidates, as the overhead involved in using a c-core would
not be offset by the computation avoided by its use.

Once 'hot' portions are identified, a c-core can be synthesised for it. While previous hardware accelerators
might have been hand-designed, such an approach is not viable if large numbers of c-cores are to be used, and so
the paper outlines a method for automating the synthesis of these cores. A control-flow graph can be extracted
from the code, and from this a state machine model can be constructed to perform the functionality represented
in the code. This state machine can then be implemented in a language such as VHDL or Verilog, and from this a circuit
can be synthesised.

\begin{figure}[h]
\center{\includegraphics[width=\textwidth]
{figures/c-cores.png}}
\caption{Conservation core example. \cite{c-cores}}
\end{figure}

One of the main issues with these c-cores is that memory synchronization restricts ILP (instruction-level parallelism)
and requires large numbers of pipeline registers (as each memory access marks a state-boundary). \cite{eco-cores} attempts
to alleviate this issue by introducing \textit{selective de-pipelining} (SDP), in which memory accesses occur in 'fast states'
while the rest of the processing done by the c-core proceeds as before, in 'slow states'.

Signals can safely propogate through the entire block without the need for latching on fast-state boundaries,
while values loaded from memory are latched on fast-state boundaries as soon as they are available.
An example of this process is shown in figure 2.4.

\begin{figure}[h]
\center{\includegraphics[width=\textwidth]
{figures/sdp.png}}
\caption{Selective de-pipelining example. \cite{eco-cores}}
\end{figure}

The GreenDroid project \cite{greendroid} takes the ideas of both \cite{c-cores} and \cite{eco-cores} and attempts
to apply them to the Android operating system and software stack. Mobile phone processors have vastly lower power budgets than
traditional desktop processors, both to achieve long battery life and to reduce generated heat. Profiling of the Android
software was used to identify the best portions of code to be converted into c-cores. As a result,
c-cores account for over 90\% of execution time, and if clock-gated when not in use (leaving them idle and consuming
very little power), this leads to a significant reduction in energy consumption.

A similar approach is taken by Arnone in \cite{arnone-thesis}. Here, Arnone outlines a method of
generating accelerator cores for a stack architecture, resulting in both timing
and power improvements. Two architectures for generated cores are described: composite and wave-core.
Composite cores are simple state machines with instructions mapped to states. Composite cores attempt to
optimise for logic area by reusing existing logic between states. Conversely, the wave-core
architecture avoids the reuse of logic between states, reducing power density at the cost of increased
logic area (due to potentially duplicated functionality). In both cases the resulting accelerator core is
more energy-efficient than the general-purpose processor is at the same task.

Here each core is a finite state machine generated from a basic block in the stack architecure's
assembly code. States are divided between computation states and memory-access states. Each computation state
is formed of a series of HDL (VHDL is used, but this could just as easily be Verilog) statements which are direct
translations of instructions in the stack architecture's instruction set. Between each computation state are one
or more states in which memory is being accessed, to either load or store data. Outputs from each state are latched
in registers so that they are available for the next state to operate on. Because each computation state is a simple
data-flow system with no need for synchronization, they can be completed within a single cycle.

These cores differ from conservation cores in that there is no control flow - while c-cores can encode loops, Arnone's
work focuses solely on basic blocks. This means that less use can be made of the cores (as the main processor must still
perform flow control), but also that the implementation of state machines is less complex - they are linear.

\chapter{Methodology and Design}

\section{System Architecture}

MicroBlaze \cite{microblaze} is a 32-bit RISC architecture. It is highly configurable, as certain features
(e.g. FPU, multiplier, barrel shifter) can be disabled if not required. This allows the architecture to be as minimal as is
required by the application. Because of this, MicroBlaze is often used in embedded environments where power efficiency is
a significant concern.

The system used in this project consists of one MicroBlaze core, connected to local instruction and data block RAM (BRAM)
via the local memory bus (LMB). The processor's AXI bus is used to connect to a hardware-accelerator control unit, which is
in turn connected to one or more hardware accelerator cores, as well as to the BRAM via a standard Xilinx BRAM interface.
All communication with memory and with the MicroBlaze core by hardware accelerators is managed by the control unit.

This reduces the number of signals required, as the accelerators themselves can use a simplified memory interface to communicate
with the memory and the MicroBlaze core, rather than each having to separately implement the LMB and AXI protocols.

\begin{figure}[h]
\center{\includegraphics[width=0.75\textwidth]
{figures/architecture.png}}
\caption{System architecture.}
\label{fig:systemArchitecture}
\end{figure}

The LMB is intended for connecting the MicroBlaze core to on-chip BRAM, for use as a local cache memory, with instructions
and data being stored in off-chip flash or DRAM. The system topology described above allows for a tiled architecture,
with each block containing a MicroBlaze core and several hardware accelerators. Each tile would have its own local memory,
with these memories being connected in turn (via some kind of cache interface) to the system-wide main memory. However,
this is beyond the scope of this project, which focuses on a single MicroBlaze core with associated accelerators,
and a single BRAM, with no main flash or DRAM.

\section{Hardware Accelerator Architecture}

Each hardware accelerator is modelled as a sequential state machine, with a sequence of states determined by the structure
of the code that the hardware accelerator is intended to replace. Each state machine block has a similar interface, as defined in table ~\ref{table:acceleratorSignals}.

\begin{table}[h]
\centering
\begin{tabular}{ |p{2cm}|p{2cm}|p{9cm}| }
\textbf{Signal} & \textbf{Direction} & \multicolumn{1}{c}{\textbf{Description}} \\
CLK             & I                  & Clock signal. \\[0.05cm]
RST             & I                  & Reset signal. Active HIGH. \\[0.05cm]
SEL             & I                  & Block-select signal. Block is active when this is HIGH, and inactive when this is LOW. \\[0.05cm]
DONE            & O                  & Done signal. This goes HIGH once the state machine has finished its computation, and goes LOW when the state machine is reset. \\[0.05cm]
M\_RDY          & I                  & Memory-ready signal. Signals to the state machine that a memory transaction has completed. \\[0.05cm]
M\_RD           & O                  & Memory read strobe. Indicates that the state machine is reading a value from memory. \\[0.05cm]
M\_WR           & O                  & Memory write strobe. Indicates that the state machine is writing a value to memory. \\[0.05cm]
M\_ADDR         & O                  & Memory address. Address of word being accessed in memory. \\[0.05cm]
M\_DATA         & I/O                & Data in/out. Data accessed in memory is written to or read from this line. \\[0.05cm]
IN\_Rxx         & I                  & One or more input register lines. Inputs to the state machine are written to these lines from the MicroBlaze core. \\[0.05cm]
OUT\_Rxx        & O                  & One or more output register lines. Outputs from the state machine are written to these lines from the state machine once it has finished computation.
\end{tabular}
\caption{Hardware accelerator input/output signals.}
\label{table:acceleratorSignals}
\end{table}

There are four kinds of states in the hardware accelerator model. The start state, which is the state the accelerator is in
when activated, transfers inputs from the accelerator's register input ports into internal registers, for use during the
computation. The end state is the state the accelerator moves to when the computation is complete. In this state, output values
are transferred from the accelerator's internal registers to the register output ports, to be read by the MicroBlaze core.
An example abstract state machine is shown in figure ~\ref{fig:abstractStateMachine}.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node[state, initial] (S_START) {S\_START};
  \node[state, right of=S_START] (S_000) {S\_000};
  \node[state, below of=S_000] (S_001) {S\_001};
  \node[state, left of=S_001] (S_002) {S\_002};
  \node[state, below of=S_001] (S_003) {S\_003};
  \node[state, left of=S_003] (S_004) {S\_004};
  \node[state, below of=S_004] (S_END) {S\_END};

  \draw (S_START) edge[above] node{CLK} (S_000);
  \draw (S_000) edge[loop right] node{CLK} (S_000);
  \draw (S_000) edge[right] node{M\_RDY} (S_001);
  \draw (S_001) edge[loop right] node{CLK} (S_001);
  \draw (S_001) edge[above] node{M\_RDY} (S_002);
  \draw (S_002) edge[above right] node{CLK} (S_003);
  \draw (S_003) edge[loop right] node{CLK} (S_003);
  \draw (S_003) edge[above] node{M\_RDY} (S_004);
  \draw (S_004) edge[right] node{CLK} (S_END);
\end{tikzpicture}
\caption{An example abstract state machine.}
\label{fig:abstractStateMachine}
\end{figure}

In between the start and end states are a series of computation and wait states. A computation state is derived from a series of
non-memory-access instructions (e.g. \texttt{addi} (add-immediate) or \texttt{mul} (integer multiply)). A circuit is constructed
from the instructions which implements the same functionality, but in parallel, as shown in figure ~\ref{fig:computationState}.
As such, while the instructions represented by a computation state might take several cycles to complete on a general-purpose
processor, the hardware accelerator can always complete a computation state in a single cycle.

\begin{figure}[H]
\center{\includegraphics[width=0.75\textwidth]
{figures/computation-state.png}}
\caption{Comparison of original program instructions and generated parallel circuit.}
\label{fig:computationState}
\end{figure}

Each computation state is separated by one or more wait states, in which the accelerator either fetches data from or writes data
to the local memory. Each wait state is associated with a single input/output instruction
(e.g. \texttt{lwi} (load word immediate) or \texttt{sb} (store byte)). These states can typically be completed within a single
cycle, but may take more depending on memory latency.

\section{Accessing Cores}

In order for generated hardware accelerators to be at all useful, a lightweight system to allow the MicroBlaze core to
activate them and retrieve results is required. The accelerator cores are connected, via a controller module, to the
general-purpose core via the AXI bus, which exposes the controller to MicroBlaze as a set of memory-mapped registers.
These registers can then be written to and read from using the \texttt{sw} (store word) and \texttt{lw} (load word) instructions.

\subsection{Using a Small-Data Pointer}

The MicroBlaze CPU has two dedicated small-data-area pointers, R2 and R13 \cite{microblaze-ref}.
A small-data area is an area of memory to which a pointer is held for the duration of an application,
allowing fast access to the region. Such a pointer can be used to hold the address of the controller module,
speeding up access to accelerator cores.

\begin{figure}[H]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{monospace}
      \# Save temporary registers.

      swi    r12, r1, -4

      swi    r11, r1, -8
\\

      \# Get pointer to controller.

      addik  r12, r0, HW\_ACCEL\_PORT
\\

      \# Write registers.

      swi    rX, r12, (X*4)

      ...
\\

      \# Activate required core.

      \# C\_id is the core's

      \# numeric ID (0-indexed).

      addik  r11, r0, (1 << C\_id)

      swi    r11, r12, 0
\\

      \# Sleep until woken.

      mbar   16
\\

      \# Read registers.

      lwi    rX, r12, (X*4)

      ...
\\

      \# Read control register.

      \# This resets the core.

      \# The value is discarded.

      lwi    r0, r12, 0
\\

      \# Restore registers.

      lwi    r11, r1, -8

      lwi    r12, r1, -4
      \end{monospace}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{monospace}
      \# Save temporary register.

      swi    r11, r1, -4
\\

      \# Pointer to controller

      \# is in R13.
\\

      \# Write registers.

      swi    rX, r13, (X*4)

      ...
\\

      \# Activate required core.

      \# C\_id is the core's

      \# numeric ID (0-indexed).

      addik  r11, r0, (1 << C\_id)

      swi    r11, r13, 0
\\

      \# Sleep until woken.

      mbar   16
\\

      \# Read registers.

      lwi    rX, r13, (X*4)

      ...
\\

      \# Read control register.

      \# This resets the core.

      \# The value is discarded.

      lwi    r0, r13, 0
\\

      \# Restore register.

      lwi    r11, r1, -4
      \end{monospace}
    \end{minipage}

  \caption{Activating a hardware accelerator core, with (right) and without (left) a small-data pointer.}
  \label{fig:smallData}
\end{figure}

As demonstrated in figure ~\ref{fig:smallData}, no temporary register needs to be used to hold a
pointer to the controller, as the pointer is already stored in the small-data register R13.
This reduces the number of instructions required to control an accelerator core.

Without the small data area, two extra instructions are required to save and restore a temporary register,
and another is needed to set up the pointer to the controller. These three instructions are eliminated with the use of a small
data pointer. This might seem insignificant, but if accelerators are used frequently, the eliminated cycles could provide
savings in both time and energy.

\section{Code Analysis}

In order to identify the best candidates for extraction into hardware accelerators, some analysis needs to be performed.
The first step in this analysis is to break the instruction stream up into basic blocks. A basic block is a linear portion of
code, bounded at both ends by flow control instructions (e.g. branch or call instructions), or a label (indicating that this
portion of code is the target of a jump somewhere else in the program). These blocks can then be linked
together into a program-flow graph, in which all nodes are basic blocks, and all edges links between those blocks.
From this representation, some information about the individual blocks can be extracted.

\subsection{Inputs and Outputs}

It is possible to know what the inputs and outputs from a block are in isolation (with an input being any register that is read
before being written to in the block, and an output being any register written to at any point in the block). However, this leads
to very conservative results, as highlighted by figure ~\ref{fig:analysisNaiveApproach}. In the example, \texttt{r6} is used as a
temporary register, and according to the MicroBlaze ABI \cite{microblaze-ref} is discarded on returning from the function. If we
take the naive approach to identifying a block's outputs, this information is ignored and it is assumed that because \texttt{r6}
is written to, it must be an output of the block. In a real block, there may be many more temporary registers used, resulting in
the analysis reporting a large number of 'false outputs'.

\begin{figure}[H]
  \begin{center}
    \begin{minipage}{0.5\linewidth}
      \begin{monospace}
      \input{figures/inputs-outputs-conservative.tex}
      \end{monospace}
    \end{minipage}
  \end{center}

  \caption{A naive approach to basic block analysis.}
  \label{fig:analysisNaiveApproach}
\end{figure}

In order to solve this problem, the analysis needs to take into account both how the registers are used (as defined by the
ABI) and the context in which the block exists (i.e. what blocks come before and after it).
The register descriptions, taken from the ABI, are shown in figure ~\ref{fig:abi}. R3 and R4 are used for returning values from
subroutines, and so must be outputs from a basic block (if written to) if that block is the last block in a subroutine
(i.e. the last instruction in it is a \texttt{rtsd} instruction). However, as R5 to R12 are volatile, they are not preserved
when returning from a subroutine (as they will be restored by the caller). Therefore, if they are written to in the last block
of a subroutine, they can be discarded as outputs.

\begin{figure}[H]
\centering
\begin{tabular}{ |p{2cm}|p{2cm}|p{9cm}| }
\textbf{Register} & \textbf{Type} & \multicolumn{1}{c}{\textbf{Description}} \\
R0       & Dedicated    & Hardcoded 0. \\[0.05cm]
R1       & Dedicated    & Stack pointer. \\[0.05cm]
R2       & Dedicated    & Read-only small-data-area pointer. \\[0.05cm]
R3-R4    & Volatile     & Return values/temporaries. \\[0.05cm]
R5-R10   & Volatile     & Passing parameters/temporaries. \\[0.05cm]
R11-R12  & Volatile     & Temporaries. \\[0.05cm]
R13      & Dedicated    & Read-write small-data-area pointer. \\[0.05cm]
R14      & Dedicated    & Return address for interrupts. \\[0.05cm]
R15      & Dedicated    & Return address for subroutines. \\[0.05cm]
R16      & Dedicated    & Return address for trap (debugger). \\[0.05cm]
R17      & Dedicated    & Return address for exceptions. \\[0.05cm]
R18      & Dedicated    & Reserved for assembler. \\[0.05cm]
R19-R31  & Non-volatile & Must be saved across function calls (callee-save).
\end{tabular}
\caption{MicroBlaze ABI register descriptions.}
\label{fig:abi}
\end{figure}

Once the true inputs and outputs of the last block of a subroutine are known, the outputs of any block directly preceeding it in
the program-flow graph in the same subroutine can be 'pruned' in a similar way. If a volatile register is an output of a
preceeding block, but not a return-value register (R3 or R4), nor an input of the following block, it is not a 'true' output and
can be assumed to be temporary. This approach can then be applied recursively until the start of the subroutine is reached.

\section{Hardware Accelerator Selection Heuristics}

There may be a limited amount of resources available for use as hardware accelerators on any given die. Therefore,
the system needs to be able to decide which code blocks should be implemented as hardware accelerators and which
should be left to execute on the general purpose core. A set of heuristics and associated rules were devised to allow
the system to make this decision in an automated fashion.

\subsection{Input/Output Overhead}

Each accelerator has a number of input and output registers, and these registers must be written to (in the case of inputs)
and read from (in the case of outputs) the accelerator core by MicroBlaze when a core is used. Each read or write is a transaction
on the AXI bus, which has an associated overhead. The more inputs or outputs a particular core is, the more expensive its
overheads will be. However, because the overhead for a block is a fixed cost, it must be considered relative to the size of the
block itself. For example, a block with 10 cycles of instructions and 2 cycles of overhead has a higher relative overhead
than a block with 100 cycles of instructions and 10 cycles of overhead.

Thus, relative I/O overhead is represented as below:

\[ \frac{cost_i + cost_o}{\#cycles} \]

Where \(cost_i\) is the cost (in cycles) of transferring inputs from MicroBlaze to the core, \(cost_o\) is the cost (in cycles)
of transferring outputs from the core to MicroBlaze, and \(\#cycles\) is the number of cycles of instructions in the core. If
this value is greater than 1, this indicates that the given block would have more I/O overhead than it has instructions,
making it a poor candidate for translation. Therefore, relative overheads >1 are undesirable, and a core becomes more optimal
the closer to 0 its relative overhead is.

\subsection{Potential Parallelism}

The purpose of the hardware accelerator cores being generated is (in part) to extract parallelism out of sequential code.
This is limited however by the maximum 'width' of any one sequence of computation instructions (i.e. instructions that operate
only on registers, and not on memory). In extremis, a single computation instruction bounded on both sides by memory-access
instructions has a 'width' of 1, and is a very poor candidate for parallelism.

The core selection process should seek to maximise potential parallelism, as represented below:

\[ \frac{\sum\limits_{c \in C} \#c}{\#C} \]

Where each \(c \in C\) is a computation sequence in the basic block, represented by \(C\), with \(\#c\) being the number of
instructions in a given sequence, and \(\#C\) being the number of sequences in a given block. Intuitively, this heuristic
represents the 'average width' of a basic block, and is a predictor of the level of parallelism that can be extracted from the
block. As such, it is expected that this heuristic be proportional to the instruction-per-cycle (IPC) count of the resulting
core.

\chapter{Results}

This section will detail results of testing and benchmarking with the system. The various benchmarks used will be described here,
and in the following sections different measurements will be presented.

\section{Speedup and Energy Savings}

Speedup: cycle count of execution of a range of benchmark applications, with varying numbers of cores (including 0, used
as the baseline for calculating speedup).

Energy saving: Comparison of post-synthesis energy estimates for baseline (0 cores) and varying number of cores, across
applications.

\section{Trends in Generated Cores}

Observe trends in generated cores across applications, e.g. input/output counts, IPC (instructions per cycle), state counts.

\section{Effectiveness of Heuristics}

Comparison of different heuristics and their impact on results of the testing. For example, difference in performance between
'simple' heuristics which evaluate basic blocks before selecting those to be converted and an 'expensive' analysis which converts
all basic blocks before comparison to see which should be implemented. Comparing performance results between these two 'modes'
could give an idea of how accurate the heuristics are.

\chapter{Conclusions and Further Work}

\section{Discussion of Results}

Discussion of the implications of the results obtained, particularly the more in-depth results such as trends and heuristics.

\section{Further Work}

Proposals of further work to be performed to extend this research.

\section{Conclusion}

\printbibliography

\end{document}